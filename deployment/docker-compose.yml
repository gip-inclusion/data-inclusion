version: "3.9"
name: "data-inclusion"

x-airflow-common:
  &airflow-common
  build:
    context: ../pipeline

  environment:
    &airflow-common-environment
    AIRFLOW__CORE__DEFAULT_TIMEZONE: Europe/Paris
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS: 'false'
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql://airflow:airflow@airflow-db:5432/airflow
    AIRFLOW__WEBSERVER__BASE_URL: http://airflow.${PUBLIC_HOSTNAME}

    # Connections
    AIRFLOW_CONN_S3: ${AIRFLOW_CONN_S3}
    AIRFLOW_CONN_PG: ${AIRFLOW_CONN_PG}

    # Variables
    AIRFLOW_VAR_DBT_PROJECT_DIR: /opt/airflow/dbt
    AIRFLOW_VAR_DBT_TARGET_PATH: /opt/airflow/dbt-runtime/target
    AIRFLOW_VAR_DBT_LOG_PATH: /opt/airflow/dbt-runtime/logs
    AIRFLOW_VAR_BAN_API_URL: ${BAN_API_URL}
    AIRFLOW_VAR_DORA_API_URL: ${DORA_API_URL}
    AIRFLOW_VAR_INSEE_FIRSTNAME_FILE_URL: ${INSEE_FIRSTNAME_FILE_URL}
    AIRFLOW_VAR_INSEE_COG_DATASET_URL: ${INSEE_COG_DATASET_URL}
    AIRFLOW_VAR_SIRENE_STOCK_ETAB_GEOCODE_FILE_URL: ${SIRENE_STOCK_ETAB_GEOCODE_FILE_URL}
    AIRFLOW_VAR_SIRENE_STOCK_ETAB_HIST_FILE_URL: ${SIRENE_STOCK_ETAB_HIST_FILE_URL}
    AIRFLOW_VAR_SIRENE_STOCK_ETAB_LIENS_SUCCESSION_URL: ${SIRENE_STOCK_ETAB_LIENS_SUCCESSION_URL}
    AIRFLOW_VAR_SIRENE_STOCK_UNITE_LEGALE_FILE_URL: ${SIRENE_STOCK_UNITE_LEGALE_FILE_URL}
    AIRFLOW_VAR_UN_JEUNE_UNE_SOLUTION_API_URL: ${UN_JEUNE_UNE_SOLUTION_API_URL}

    # make the data_inclusion package available in editable mode
    PYTHONPATH: $${PYTHONPATH}:/opt/airflow/data-inclusion/src

  volumes:
    - airflow-logs:/opt/airflow/logs
    - ../pipeline/dbt:/opt/airflow/dbt:ro
    - ../pipeline/dags:/opt/airflow/dags:ro
    - ../pipeline/src:/opt/airflow/data-inclusion/src:ro

  depends_on:
    &airflow-common-depends-on
    airflow-db:
      condition: service_healthy

services:
  airflow-db:
    image: postgres:14
    restart: on-failure
    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "airflow" ]
      interval: 5s
      retries: 5
    environment:
      - POSTGRES_DB=airflow
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash -c
    user: 0:0
    command:
      - |
        mkdir -p /opt/airflow/logs
        chown -R "50000:0" /opt/airflow/logs
        exec /entrypoint airflow version
    environment:
      <<: *airflow-common-environment
      # Additional variables for development only
      # TODO: replace with `airflow db upgrade`
      _AIRFLOW_DB_UPGRADE: 'true'
      # TODO: replace with `airflow users create`
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: airflow
      _AIRFLOW_WWW_USER_PASSWORD: ${AIRFLOW_WWW_USER_PASSWORD}

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    restart: on-failure
    ports:
      - 8080:8080
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.airflow.rule=Host(`airflow.${PUBLIC_HOSTNAME}`)"
      - "traefik.http.routers.airflow.entrypoints=http"
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    restart: on-failure
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"'
        ]
      interval: 10s
      timeout: 10s
      retries: 5
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  dbt-init:
    image: ghcr.io/dbt-labs/dbt-postgres:1.5.1
    command: deps
    environment:
      DBT_PROFILES_DIR: /usr/app
    volumes:
      - ../pipeline/dbt:/usr/app

  datawarehouse:
    image: postgis/postgis:14-3.3
    restart: on-failure
    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "${DATAWAREHOUSE_DI_USERNAME}"]
      interval: 5s
      retries: 5
    ports:
      - 5432:5432
    environment:
      - POSTGRES_DB=${DATAWAREHOUSE_DI_DATABASE}
      - POSTGRES_USER=${DATAWAREHOUSE_DI_USERNAME}
      - POSTGRES_PASSWORD=${DATAWAREHOUSE_DI_PASSWORD}
    volumes:
      - datawarehouse-data:/var/lib/postgresql/data

  api:
    image: ghcr.io/betagouv/data-inclusion-api:${API_VERSION}
    depends_on:
      - datawarehouse
    restart: always
    ports:
      - 8000:8000
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.api.rule=Host(`api.${PUBLIC_HOSTNAME}`)"
      - "traefik.http.routers.api.entrypoints=http"
    environment:
      - ENV=prod
      - DEBUG=False
      - DATABASE_URL=${AIRFLOW_CONN_PG}  # TODO: create dedicated RO creds for api
      - SECRET_KEY=${API_SECRET_KEY}
      - ROOT_PATH=/api

  reverse-proxy:
    image: traefik:v2.10
    ports:
      - 80:80
      - 8081:8080
    environment:
      - TRAEFIK_PROVIDERS_DOCKER=true
      - TRAEFIK_PROVIDERS_DOCKER_EXPOSEDBYDEFAULT=false
      # enable the web ui (disabled for prod)
      - TRAEFIK_API_INSECURE=true
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro

volumes:
  airflow-logs:
  datawarehouse-data:

networks:
  default:
    name: data-inclusion
