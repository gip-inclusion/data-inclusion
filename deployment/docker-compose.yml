# Based on https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#fetching-docker-compose-yaml

name: "data-inclusion"

x-airflow-common:
  &airflow-common

  environment:
    &airflow-common-environment
    AIRFLOW__CORE__DEFAULT_TIMEZONE: Europe/Paris
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
    AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS: 'false'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__CORE__MAX_MAP_LENGTH: 2048
    AIRFLOW__CORE__PARALLELISM: 4
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql://airflow:airflow@airflow-db:5432/airflow
    AIRFLOW__SCHEDULER__MAX_TIS_PER_QUERY: 0
    AIRFLOW__SENTRY__RELEASE: ${AIRFLOW__SENTRY__RELEASE}
    AIRFLOW__SENTRY__SENTRY_DSN: ${AIRFLOW__SENTRY__SENTRY_DSN}
    AIRFLOW__SENTRY__SENTRY_ON: 'true'
    AIRFLOW__SENTRY__TRACES_SAMPLE_RATE: 0.1
    AIRFLOW__SENTRY__BEFORE_SEND: data_inclusion.pipeline.common.sentry.before_send
    AIRFLOW__WEBSERVER__BASE_URL: ${AIRFLOW__WEBSERVER__BASE_URL}
    # the following var should be dropped in airflow 3
    AIRFLOW__METRICS__TIMER_UNIT_CONSISTENCY: 'true'

    # Connections
    AIRFLOW_CONN_PG: ${AIRFLOW_CONN_PG}
    AIRFLOW_CONN_S3: ${AIRFLOW_CONN_S3}

    # Variables
    AIRFLOW_VAR_BREVO_API_KEY: ${AIRFLOW_VAR_BREVO_API_KEY}
    AIRFLOW_VAR_CARIF_OREF_URL: ${AIRFLOW_VAR_CARIF_OREF_URL}
    AIRFLOW_VAR_DATA_INCLUSION_API_PROBE_TOKEN: ${AIRFLOW_VAR_DATA_INCLUSION_API_PROBE_TOKEN}
    AIRFLOW_VAR_DATAGOUV_API_KEY: ${AIRFLOW_VAR_DATAGOUV_API_KEY}
    AIRFLOW_VAR_DORA_API_TOKEN: ${AIRFLOW_VAR_DORA_API_TOKEN}
    AIRFLOW_VAR_DORA_API_URL: ${AIRFLOW_VAR_DORA_API_URL}
    AIRFLOW_VAR_EMPLOIS_API_TOKEN: ${AIRFLOW_VAR_EMPLOIS_API_TOKEN}
    AIRFLOW_VAR_ENVIRONMENT: ${AIRFLOW_VAR_ENVIRONMENT}
    AIRFLOW_VAR_FREDO_API_TOKEN: ${AIRFLOW_VAR_FREDO_API_TOKEN}
    AIRFLOW_VAR_MISSION_LOCALE_API_SECRET: ${AIRFLOW_VAR_MISSION_LOCALE_API_SECRET}
    AIRFLOW_VAR_FT_API_TOKEN: ${AIRFLOW_VAR_FT_API_TOKEN}
    AIRFLOW_VAR_MEDIATION_NUMERIQUE_API_TOKEN: ${AIRFLOW_VAR_MEDIATION_NUMERIQUE_API_TOKEN}
    AIRFLOW_VAR_MES_AIDES_AIRTABLE_KEY: ${AIRFLOW_VAR_MES_AIDES_AIRTABLE_KEY}
    AIRFLOW_VAR_SOLIGUIDE_API_TOKEN: ${AIRFLOW_VAR_SOLIGUIDE_API_TOKEN}
    AIRFLOW_VAR_TWOCAPTCHA_API_KEY: ${AIRFLOW_VAR_TWOCAPTCHA_API_KEY}
    AIRFLOW_VAR_MA_BOUSSOLE_AIDANTS_API_KEY: ${AIRFLOW_VAR_MA_BOUSSOLE_AIDANTS_API_KEY}

  volumes:
    - airflow-logs:/opt/airflow/logs

  depends_on:
    &airflow-common-depends-on
    airflow-db:
      condition: service_healthy

services:
  airflow-db:
    image: postgres:14
    restart: always
    healthcheck:
      test: pg_isready
      interval: 5s
      retries: 5
    environment:
      - PGUSER=airflow
      - POSTGRES_DB=airflow
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow

  airflow-init:
    <<: *airflow-common
    image: ghcr.io/gip-inclusion/data-inclusion-pipeline:${STACK_VERSION}
    command: version
    environment:
      <<: *airflow-common-environment
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: airflow
      _AIRFLOW_WWW_USER_PASSWORD: ${AIRFLOW_WWW_USER_PASSWORD}

  airflow-webserver:
    <<: *airflow-common
    image: ghcr.io/gip-inclusion/data-inclusion-pipeline:${STACK_VERSION}
    command: webserver
    restart: always
    healthcheck:
      test: curl --fail http://localhost:8080/health
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    ports:
      - 8080:8080
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.airflow.rule=Host(`${AIRFLOW_HOSTNAME}`)"
      - "traefik.http.routers.airflow.entrypoints=websecure"
      - "traefik.http.routers.airflow.tls.certresolver=main"
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    image: ghcr.io/gip-inclusion/data-inclusion-pipeline:${STACK_VERSION}
    command: scheduler
    restart: always
    healthcheck:
      test: airflow jobs check --job-type SchedulerJob --local
      interval: 10s
      timeout: 20s
      retries: 5
      start_period: 30s
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  playwright:
    # the version should match the tasks requirements
    image: mcr.microsoft.com/playwright:v1.55.0-noble
    command: npx -y playwright@1.55.0 run-server --port 3000 --host 0.0.0.0
    user: pwuser
    init: true
    ipc: host
    depends_on:
      - airflow-scheduler

  datawarehouse:
    image: ghcr.io/gip-inclusion/data-inclusion-datawarehouse:${STACK_VERSION}
    restart: always
    healthcheck:
      test: pg_isready
      interval: 5s
      retries: 5
    ports:
      - 5432:5432
    environment:
      - PGUSER=${DATAWAREHOUSE_DI_USERNAME}
      - POSTGRES_DB=${DATAWAREHOUSE_DI_DATABASE}
      - POSTGRES_USER=${DATAWAREHOUSE_DI_USERNAME}
      - POSTGRES_PASSWORD=${DATAWAREHOUSE_DI_PASSWORD}
      - BREVO_API_KEY=${AIRFLOW_VAR_BREVO_API_KEY}
    volumes:
      - datawarehouse-data:/var/lib/postgresql/data

  reverse-proxy:
    image: traefik:v3.4
    restart: always
    ports:
      - 80:80
      - 443:443
    environment:
      - TRAEFIK_PROVIDERS_DOCKER=true
      - TRAEFIK_PROVIDERS_DOCKER_EXPOSEDBYDEFAULT=false
      - TRAEFIK_ENTRYPOINTS_WEB_ADDRESS=:80
      - TRAEFIK_ENTRYPOINTS_WEB_HTTP_REDIRECTIONS_ENTRYPOINT_TO=websecure
      - TRAEFIK_ENTRYPOINTS_WEBSECURE_ADDRESS=:443
      - TRAEFIK_CERTIFICATESRESOLVERS_MAIN_ACME_TLSCHALLENGE=true
      - TRAEFIK_CERTIFICATESRESOLVERS_MAIN_ACME_EMAIL=tech@data.inclusion.beta.gouv.fr
      - TRAEFIK_CERTIFICATESRESOLVERS_MAIN_ACME_STORAGE=/letsencrypt/acme.json
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - letsencrypt-data:/letsencrypt

volumes:
  airflow-logs:
  datawarehouse-data:
  letsencrypt-data:

networks:
  default:
    name: data-inclusion
