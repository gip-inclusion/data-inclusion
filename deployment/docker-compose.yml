# Based on https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#fetching-docker-compose-yaml

version: "3"
name: "data-inclusion"

x-airflow-common:
  &airflow-common

  environment:
    &airflow-common-environment
    AIRFLOW__CORE__DEFAULT_TIMEZONE: Europe/Paris
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS: 'false'
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql://airflow:airflow@airflow-db:5432/airflow
    AIRFLOW__WEBSERVER__BASE_URL: https://airflow.${PUBLIC_HOSTNAME}

    # Connections
    AIRFLOW_CONN_S3: ${AIRFLOW_CONN_S3}
    AIRFLOW_CONN_PG: ${AIRFLOW_CONN_PG}

    # Variables
    AIRFLOW_VAR_DBT_PROJECT_DIR: /opt/airflow/dbt
    AIRFLOW_VAR_DBT_TARGET_PATH: /opt/airflow/dbt-runtime/target
    AIRFLOW_VAR_DBT_LOG_PATH: /opt/airflow/dbt-runtime/logs
    AIRFLOW_VAR_BAN_API_URL: ${BAN_API_URL}
    AIRFLOW_VAR_DORA_API_URL: ${DORA_API_URL}
    AIRFLOW_VAR_DORA_API_TOKEN: ${DORA_API_TOKEN}
    AIRFLOW_VAR_INSEE_FIRSTNAME_FILE_URL: ${INSEE_FIRSTNAME_FILE_URL}
    AIRFLOW_VAR_INSEE_COG_DATASET_URL: ${INSEE_COG_DATASET_URL}
    AIRFLOW_VAR_SIRENE_STOCK_ETAB_GEOCODE_FILE_URL: ${SIRENE_STOCK_ETAB_GEOCODE_FILE_URL}
    AIRFLOW_VAR_SIRENE_STOCK_ETAB_HIST_FILE_URL: ${SIRENE_STOCK_ETAB_HIST_FILE_URL}
    AIRFLOW_VAR_SIRENE_STOCK_ETAB_LIENS_SUCCESSION_URL: ${SIRENE_STOCK_ETAB_LIENS_SUCCESSION_URL}
    AIRFLOW_VAR_SIRENE_STOCK_UNITE_LEGALE_FILE_URL: ${SIRENE_STOCK_UNITE_LEGALE_FILE_URL}
    AIRFLOW_VAR_UN_JEUNE_UNE_SOLUTION_API_URL: ${UN_JEUNE_UNE_SOLUTION_API_URL}

  volumes:
    - airflow-logs:/opt/airflow/logs

  depends_on:
    &airflow-common-depends-on
    airflow-db:
      condition: service_healthy

services:
  airflow-db:
    image: postgres:14
    restart: always
    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "airflow" ]
      interval: 5s
      retries: 5
    environment:
      - POSTGRES_DB=airflow
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow

  airflow-init:
    <<: *airflow-common
    image: apache/airflow:2.7.0-python3.10
    command: version
    environment:
      <<: *airflow-common-environment
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: airflow
      _AIRFLOW_WWW_USER_PASSWORD: ${AIRFLOW_WWW_USER_PASSWORD}

  airflow-webserver:
    <<: *airflow-common
    image: apache/airflow:2.7.0-python3.10
    command: webserver
    restart: always
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    ports:
      - 8080:8080
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.airflow.rule=Host(`airflow.${PUBLIC_HOSTNAME}`)"
      - "traefik.http.routers.airflow.entrypoints=websecure"
      - "traefik.http.routers.airflow.tls.certresolver=main"
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    image: ghcr.io/betagouv/data-inclusion-pipeline:${STACK_VERSION}
    command: scheduler
    restart: always
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"'
        ]
      interval: 10s
      timeout: 10s
      retries: 5
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  datawarehouse:
    image: ghcr.io/betagouv/data-inclusion-datawarehouse:${STACK_VERSION}
    restart: always
    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "${DATAWAREHOUSE_DI_USERNAME}"]
      interval: 5s
      retries: 5
    ports:
      - 5432:5432
    environment:
      - POSTGRES_DB=${DATAWAREHOUSE_DI_DATABASE}
      - POSTGRES_USER=${DATAWAREHOUSE_DI_USERNAME}
      - POSTGRES_PASSWORD=${DATAWAREHOUSE_DI_PASSWORD}
    volumes:
      - datawarehouse-data:/var/lib/postgresql/data

  api:
    image: ghcr.io/betagouv/data-inclusion-api:${STACK_VERSION}
    depends_on:
      - datawarehouse
    restart: always
    ports:
      - 8000:8000
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.api.rule=Host(`api.${PUBLIC_HOSTNAME}`)"
      - "traefik.http.routers.api.entrypoints=websecure"
      - "traefik.http.routers.api.tls.certresolver=main"
    environment:
      - ENV=prod
      - DEBUG=False
      - DATABASE_URL=${AIRFLOW_CONN_PG}  # TODO: create dedicated RO creds for api
      - SECRET_KEY=${API_SECRET_KEY}
      - ROOT_PATH=/api
      - TOKEN_ENABLED=${API_TOKEN_ENABLED}

  reverse-proxy:
    image: traefik:v2.10
    ports:
      - 80:80
      - 443:443
      - 8081:8080
    environment:
      # - TRAEFIK_API_INSECURE=true
      # - TRAEFIK_LOG_LEVEL=debug
      - TRAEFIK_PROVIDERS_DOCKER=true
      - TRAEFIK_PROVIDERS_DOCKER_EXPOSEDBYDEFAULT=false
      - TRAEFIK_ENTRYPOINTS_WEB_ADDRESS=:80
      - TRAEFIK_ENTRYPOINTS_WEB_HTTP_REDIRECTIONS_ENTRYPOINT_TO=websecure
      - TRAEFIK_ENTRYPOINTS_WEBSECURE_ADDRESS=:443
      - TRAEFIK_CERTIFICATESRESOLVERS_MAIN_ACME_TLSCHALLENGE=true
      - TRAEFIK_CERTIFICATESRESOLVERS_MAIN_ACME_EMAIL=tech@data.inclusion.beta.gouv.fr
      - TRAEFIK_CERTIFICATESRESOLVERS_MAIN_ACME_STORAGE=/letsencrypt/acme.json
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - letsencrypt-data:/letsencrypt

volumes:
  airflow-logs:
  datawarehouse-data:
  letsencrypt-data:

networks:
  default:
    name: data-inclusion
