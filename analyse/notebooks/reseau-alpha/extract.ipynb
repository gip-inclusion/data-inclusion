{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# URL = \"https://www.reseau-alpha.org/trouver-une-formation?form%5BcodePostal%5D%5B%5D=%7C91&form%5BcriteresScolarisation%5D=&form%5BniveauLinguistiqueVise%5D=&form%5Bprogramme%5D=&form%5BmotCle%5D=\"\n",
    "URL = 'file:///home/colin/git/data-inclusion/analyse/notebooks/reseau-alpha/structure-list.html'\n",
    "\n",
    "# Live HTML (don't use too much!)\n",
    "# structure_base_url = 'https://www.reseau-alpha.org/structure/apprentissage-du-francais/'\n",
    "\n",
    "# Local HTML\n",
    "structure_base_url = \"file:///home/colin/git/data-inclusion/analyse/notebooks/reseau-alpha/structures/\"\n",
    "\n",
    "class AlphaSpider(scrapy.Spider):\n",
    "    name = \"alpha\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            URL\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        dataslugs = response.css('div#div-accordion-structure > h3').xpath('@data-slug').getall()\n",
    "\n",
    "        for slug in dataslugs:\n",
    "            next_page = structure_base_url + slug\n",
    "            yield scrapy.Request(next_page, callback=self.parse_structure)\n",
    "    \n",
    "    def parse_structure(self, response):\n",
    "        slug = response.url.split('/')[-1]\n",
    "        filename = f\"structures/{slug}\"\n",
    "        # Path(filename).write_bytes(response.body)\n",
    "\n",
    "        adresse_parts = response.css('div.lieu div.adresse::text').getall()\n",
    "\n",
    "\n",
    "        code_postal = adresse = \"\"\n",
    "        \n",
    "        # Adresse\n",
    "        clean_adresse_parts = []\n",
    "        for part in adresse_parts:\n",
    "            print(part.strip())\n",
    "            if re.match(r'^\\d}', part):\n",
    "                if re.match(r'^\\d{5}', part):\n",
    "                    code_postal = part.strip()[0:5]\n",
    "                else:\n",
    "                    adresse = part.strip()\n",
    "            clean_adresse_parts.append(part.strip())\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "        # Téléphone\n",
    "        telephone = response.css('div.lieu div.telephone > a::attr(href)').get()\n",
    "        if type(telephone) == str:\n",
    "            telephone = telephone.strip()[4:]\n",
    "        else:\n",
    "            telephone = \"\"\n",
    "\n",
    "        yield {\n",
    "            \"structure_id\": slug,\n",
    "            \"structure_name\": response.css('div#structure > strong::text').get().strip(),\n",
    "            \"code_postal\": code_postal,\n",
    "            \"adresse\": adresse,\n",
    "            \"adresse_entière\": clean_adresse_parts,\n",
    "            \"site_web\": response.css('div.lieu div.facebook::text').get().strip(),\n",
    "            \"telephone\": telephone,\n",
    "\n",
    "        }\n",
    "            \n",
    "    \n",
    "process = CrawlerProcess(settings={\n",
    "    \"FEEDS\": {\n",
    "        \"structures.csv\": {\n",
    "            \"format\": \"csv\",\n",
    "            \"overwrite\": True,\n",
    "            },\n",
    "        \n",
    "    },\n",
    "})\n",
    "process.crawl(AlphaSpider)\n",
    "process.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-analyse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
